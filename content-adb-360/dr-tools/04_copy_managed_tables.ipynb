{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b0d3110-4bc8-4859-8661-e69d0448f92c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Notebook to synch the tables from source to destination UC\n",
    "---\n",
    "This notebook is going to create a share in the secondary workspace and then deep cloning the source data, which is defined in the dict in the variables section.\n",
    "\n",
    "Source Workspace : uri to source workspace\n",
    "\n",
    "Destination Workspace : uri to destination workspace\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "399cd139-a73b-403e-b490-beaeb8b48aa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"source_workspace\", \"\", \"Source Workspace\")\n",
    "dbutils.widgets.text(\"destination_workspace\", \"\", \"Destination Workspace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "888f2819-e185-466b-b0e4-2751438c3712",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source_workspace = dbutils.widgets.get(\"source_workspace\")\n",
    "destination_workspace = dbutils.widgets.get(\"destination_workspace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1388cb4b-7179-4af6-9800-5da9f659f09d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get the credentials for sp from a keyvault backed secret scope\n",
    "clientid = dbutils.secrets.get('kvbacked', 'clientid')\n",
    "clientsecret = dbutils.secrets.get('kvbacked', 'clientsecret')\n",
    "tenantid = dbutils.secrets.get('kvbacked', 'tenantid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abf0d8c8-8e4e-4da3-bc89-e49ceb962af2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#variables\n",
    "# catalogs to copy is the catalog name with the target storage root\n",
    "cats_to_copy = [{\n",
    "    \"catadb360dev\" : {\n",
    "                        \"schemas\" : [\n",
    "                            {\"schema\" : \"schemaadb360dev\"},\n",
    "                            {\"schema\" : \"silverdb\"},\n",
    "                            {\"schema\" : \"golddb\"}\n",
    "                        ]\n",
    "       }\n",
    "    }              \n",
    "]\n",
    "\n",
    "warehouse = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4eddcb87-ee95-42ac-bfcb-379ada3b887b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get the catalogs to work on\n",
    "catalog_names = [list(cat.keys())[0] for cat in cats_to_copy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9f54fc3-6bbd-480b-8c7d-a80040e16e82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# for a specific catalog, get the schemas\n",
    "catalog_name = \"catadb360dev\"\n",
    "schema_names = [schema[\"schema\"] for cat in cats_to_copy if catalog_name in cat for schema in cat[catalog_name][\"schemas\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5212b608-1711-48db-ac13-835dbc3f5e6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.errors.platform import ResourceAlreadyExists, BadRequest\n",
    "from databricks.sdk.service import catalog\n",
    "from databricks.sdk.service.catalog import Privilege, PermissionsChange\n",
    "from databricks.sdk.service.sharing import (AuthenticationType, SharedDataObjectUpdate,\n",
    "                                            SharedDataObjectUpdateAction, SharedDataObject,\n",
    "                                            SharedDataObjectDataObjectType, SharedDataObjectStatus)\n",
    "from databricks.sdk.service.sql import (Disposition, StatementState,\n",
    "                                        CreateWarehouseRequestWarehouseType, ExecuteStatementRequestOnWaitTimeout)\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2293a47-dc5e-4102-aac4-3059d7f17fdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# creating the sdk connection to source and destination workspace\n",
    "sourceWs = WorkspaceClient(azure_client_id=clientid, azure_client_secret=clientsecret, azure_tenant_id=tenantid, host=source_workspace)\n",
    "destWs = WorkspaceClient(azure_client_id=clientid, azure_client_secret=clientsecret, azure_tenant_id=tenantid, host=destination_workspace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ed16b7a-9b1f-4e26-b478-3a415e761125",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create a recipient in source workspace\n",
    "# get global metastore id for destination\n",
    "dest_metastore_id = destWs.metastores.list()[0].global_metastore_id\n",
    "\n",
    "try:\n",
    "    recipient = sourceWs.recipients.create(\n",
    "        name=\"dr_recipient\",\n",
    "        authentication_type=AuthenticationType.DATABRICKS,\n",
    "        data_recipient_global_metastore_id=dest_metastore_id\n",
    "    )\n",
    "    print(f\"recipient created for metastore: {dest_metastore_id}\")\n",
    "except (ResourceAlreadyExists, BadRequest):\n",
    "    recipient = sourceWs.recipients.get(name=\"dr_recipient\")\n",
    "    print(f\"recipient already exists for metastore: {dest_metastore_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa84f010-b5d4-4f41-b423-14fe7a6f1f48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get all the tables in the primary metastore\n",
    "system_info_df = spark.sql(\"Select * from system.information_schema.tables  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba87c9ba-33b9-4760-8268-0e7335c99be7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get remote provider name\n",
    "try:\n",
    "    # get local metastore id\n",
    "    local_metastore_id = [r[\"current_metastore()\"] for r in spark.sql(\"SELECT current_metastore()\").collect()][0]\n",
    "    remote_provider_name = [p.name for p in destWs.providers.list() if\n",
    "                            p.data_provider_global_metastore_id == local_metastore_id][0]\n",
    "    print(f'found remote provider: {remote_provider_name}')\n",
    "except IndexError:\n",
    "    print(\"Provider could not be found in target workspace; please check that it was created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c89eaf02-2187-457d-93c5-cbfabfce74a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# create the destination warehouse\n",
    "try:\n",
    "    warehouse = destWs.warehouses.create(\n",
    "         name='dr_warehouse',\n",
    "         cluster_size=\"2X-Small\",\n",
    "         max_num_clusters=1,\n",
    "         auto_stop_mins=10,\n",
    "         warehouse_type=CreateWarehouseRequestWarehouseType(\"PRO\"),\n",
    "         enable_serverless_compute=True,\n",
    "    )\n",
    "    print('warehouse dr_warehouse successfully created in destination workspace')\n",
    "except BadRequest:\n",
    "    warehouse = [wh for wh in destWs.warehouses.list() if wh.name == 'dr_warehouse'][0]\n",
    "    print(\"Warehouse already exists; please delete it before running this script again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "629c4bb9-f1ee-4b6e-93de-5a1e1c29c881",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# define clone_table function\n",
    "def clone_a_table(wsc: WorkspaceClient, source_catalog: str, dest_catalog: str, schema: str, table_name: str, whid: str ):\n",
    "    print(f'cloning table {source_catalog}.{schema}.{table_name}..to {dest_catalog}.')\n",
    "\n",
    "    try:\n",
    "        sqlstmt = f'CREATE OR REPLACE TABLE {dest_catalog}.{schema}.{table_name} DEEP CLONE {source_catalog}.{schema}.{table_name}'\n",
    "\n",
    "        resp = wsc.statement_execution.execute_statement(\n",
    "            warehouse_id=whid,\n",
    "            on_wait_timeout=ExecuteStatementRequestOnWaitTimeout(\"CONTINUE\"),\n",
    "            disposition=Disposition(\"EXTERNAL_LINKS\"),\n",
    "            statement=sqlstmt\n",
    "        )\n",
    "\n",
    "        while resp.status.state in {StatementState.PENDING, StatementState.RUNNING}:\n",
    "            resp = wsc.statement_execution.get_statement(resp.statement_id)\n",
    "            time.sleep(5)\n",
    "\n",
    "        if resp.status.state != StatementState.SUCCEEDED:\n",
    "            return {\"catalog\": dest_catalog,\n",
    "                    \"schema\": schema,\n",
    "                    \"table_name\": table_name,\n",
    "                    \"status\": f\"FAIL: {resp.status.error.message}\"}\n",
    "        else:\n",
    "            return {\"catalog\": dest_catalog,\n",
    "                    \"schema\": schema,\n",
    "                    \"table_name\": table_name,\n",
    "                    \"status\": \"SUCCESS\"}\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"catalog\": dest_catalog,\n",
    "                \"schema\": schema,\n",
    "                \"table_name\": table_name,\n",
    "                \"status\": f\"FAIL: {e}\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0775050-74c6-4330-8902-17612ce88a04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# main loop for catalogs\n",
    "for catalog_name in catalog_names:\n",
    "    print(f'working on catalog {catalog_name}')\n",
    "    # filter tables dataframe\n",
    "    filtered_tables = system_info_df.filter((system_info_df.table_catalog == catalog_name) &\n",
    "                            (system_info_df.table_schema != \"information_schema\") &\n",
    "                            (system_info_df.table_type != \"VIEW\")).distinct().collect()\n",
    "\n",
    "    unique_schemas = {row['table_schema'] for row in filtered_tables}\n",
    "    all_tables = [row[\"table_name\"] for row in filtered_tables]\n",
    "    all_schemas = [row[\"table_schema\"] for row in filtered_tables]             \n",
    "\n",
    "    print(f'creating share for catalog {catalog_name}')\n",
    "    try:\n",
    "        share = sourceWs.shares.create(\n",
    "            name=f\"{catalog_name}_share\"\n",
    "        )\n",
    "        share_name = share.name\n",
    "        print(f'share created: {share_name}')\n",
    "    except BadRequest:\n",
    "        print(f'share already exists: {catalog_name}_share')\n",
    "        share_name = f\"{catalog_name}_share\"\n",
    "\n",
    "    try:\n",
    "        _ = sourceWs.shares.update_permissions(\n",
    "            share_name,\n",
    "            changes=[PermissionsChange(add=[Privilege.SELECT], principal=recipient.name)]\n",
    "        ) \n",
    "        print(f'updated share permissions for {share_name}')\n",
    "    except BadRequest:\n",
    "        print(f'could not update permissions for share {share_name}')\n",
    " \n",
    "    # build update object with all schemas in the current catalog\n",
    "    updates = [\n",
    "        SharedDataObjectUpdate(action=SharedDataObjectUpdateAction.ADD,\n",
    "                               data_object=SharedDataObject(name=f\"{catalog_name}.{schema}\",\n",
    "                                                            data_object_type=SharedDataObjectDataObjectType.SCHEMA,\n",
    "                                                            status=SharedDataObjectStatus.ACTIVE))\n",
    "        for schema in unique_schemas]\n",
    "    \n",
    "    # update the share\n",
    "    try:\n",
    "        _ = sourceWs.shares.update(share_name, updates=updates)\n",
    "        print(f'Updated share {share_name} successfully') \n",
    "    except Exception as e:\n",
    "        print(f\"Error updating share {share_name}: {e}\")\n",
    "\n",
    "    # create the shared catalog in the target workspace\n",
    "    try:\n",
    "        _ = destWs.catalogs.create(name=f\"{catalog_name}_share\", provider_name=remote_provider_name, share_name=share_name)\n",
    "        print(f\"Created shared catalog {catalog_name}_share successfully.\")\n",
    "    except BadRequest:\n",
    "        print(f\"Shared catalog {catalog_name}_share already exists. Skipping creation.\")\n",
    "\n",
    "    for i in range(len(all_tables)):\n",
    "        #print(f'clone_a_table(destWs, {catalog_name}, {catalog_name+\"_share\"}, {all_schemas[i]}, {all_tables[i]}')\n",
    "        r = clone_a_table(destWs, catalog_name+\"_share\", catalog_name, all_schemas[i], all_tables[i], warehouse.id)\n",
    "        print(r)\n",
    "    \n",
    "print('finished clone')\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_copy_managed_tables",
   "widgets": {
    "destination_workspace": {
     "currentValue": "https://adb-4274552575873366.6.azuredatabricks.net/",
     "nuid": "59f517a6-53a6-4f27-bc8c-64e4bb31f9fd",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Destination Workspace",
      "name": "destination_workspace",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Destination Workspace",
      "name": "destination_workspace",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "source_workspace": {
     "currentValue": "https://adb-2154722801623435.15.azuredatabricks.net/",
     "nuid": "bb8af541-48e2-44b6-bac0-0072286fcb4b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Source Workspace",
      "name": "source_workspace",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Source Workspace",
      "name": "source_workspace",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
